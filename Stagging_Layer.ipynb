{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac56dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578b9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://postgres:123456@localhost:5432/Project1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DB: postgresql://postgres:***@localhost:5432/Project1\n",
      "Writing to silver1.cleaned\n",
      "[GLOBAL] Will drop 12 columns with >50.0% nulls: ['college_house', 'num_living_adults', 'ctelnum1', 'cellfon2', 'cadult', 'private_residence_2', 'college_house_2', 'cstate', 'landline', 'household_adults', 'bp_meds', 'asthma_now']\n",
      "[GLOBAL] Kept columns: ['state', 'survey_month', 'survey_date', 'month', 'day', 'year', 'disposition_code', 'sequence_number', 'primary_sampling_unit', 'telephone_number', 'private_residence', 'state_residence', 'cell_phone', 'num_of_adults', 'num_of_men', 'num_of_women', 'general_health', 'physical_health_days', 'mental_health_days', 'poor_health_days', 'has_health_plan', 'has_personal_doctor', 'medical_cost_issue', 'last_checkup', 'high_blood_pressure', 'high_cholesterol', 'cholesterol_check', 'diagnosed_diabetes', 'had_heart_attack', 'had_coronary_heart_disease', 'had_stroke', 'has_asthma', 'had_skin_cancer', 'had_other_cancer', 'has_copd', 'has_arthritis', 'has_depression', 'had_kidney_disease']\n",
      "[INIT] Creating/replacing silver1.cleaned with kept columns only...\n",
      "Offset 0: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 100,000.\n",
      "Offset 100000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 200,000.\n",
      "Offset 200000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 300,000.\n",
      "Offset 300000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 400,000.\n",
      "Offset 400000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 500,000.\n",
      "Offset 500000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 600,000.\n",
      "Offset 600000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 700,000.\n",
      "Offset 700000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 800,000.\n",
      "Offset 800000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 900,000.\n",
      "Offset 900000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,000,000.\n",
      "Offset 1000000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,100,000.\n",
      "Offset 1100000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,200,000.\n",
      "Offset 1200000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,300,000.\n",
      "Offset 1300000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,400,000.\n",
      "Offset 1400000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,500,000.\n",
      "Offset 1500000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,600,000.\n",
      "Offset 1600000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,700,000.\n",
      "Offset 1700000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,800,000.\n",
      "Offset 1800000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 1,900,000.\n",
      "Offset 1900000: fetched 100000 -> cleaned 100000 -> written 100000. Total written: 2,000,000.\n",
      "✅ Silver layer processing completed!\n",
      "Total rows written: 2,000,000\n",
      "Available schemas: ['bronze1', 'gold1', 'information_schema', 'public', 'silver', 'silver1']\n",
      "Tables in schema 'silver1': ['cleaned']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.types import Integer, BigInteger, Date\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "batch_size = 100000\n",
    "offset = 0\n",
    "total_rows = 2000000\n",
    "\n",
    "source_schema = 'bronze1'\n",
    "source_table = 'raw'\n",
    "target_schema = 'silver1'\n",
    "target_table = 'cleaned'   \n",
    "\n",
    "selected_cols = [\n",
    "    '_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU',\n",
    "    'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES', 'CELLFON3', 'LADULT', 'NUMADULT',\n",
    "    'NUMMEN', 'NUMWOMEN', 'CTELNUM1', 'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS',\n",
    "    'CSTATE', 'LANDLINE', 'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH',\n",
    "    'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO',\n",
    "    'CHOLCHK', 'TOLDHI2', 'CVDINFR4', 'CVDCRHD4', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW',\n",
    "    'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY'\n",
    "]\n",
    "cols_str = ', '.join(selected_cols)\n",
    "\n",
    "rename_map = {\n",
    "    '_state': 'state',\n",
    "    'fmonth': 'survey_month',\n",
    "    'idate': 'survey_date',\n",
    "    'imonth': 'month',\n",
    "    'iday': 'day',\n",
    "    'iyear': 'year',\n",
    "    'dispcode': 'disposition_code',\n",
    "    'seqno': 'sequence_number',\n",
    "    '_psu': 'primary_sampling_unit',\n",
    "    'ctelenum': 'telephone_number',\n",
    "    'pvtresd1': 'private_residence',\n",
    "    'colghous': 'college_house',\n",
    "    'stateres': 'state_residence',\n",
    "    'cellfon3': 'cell_phone',\n",
    "    'ladult': 'num_living_adults',\n",
    "    'numadult': 'num_of_adults',\n",
    "    'nummen': 'num_of_men',\n",
    "    'numwomen': 'num_of_women',\n",
    "    'ctelnum1': 'ctelnum1',\n",
    "    'cellfon2': 'cellfon2',\n",
    "    'cadult': 'cadult',\n",
    "    'pvtresd2': 'private_residence_2',\n",
    "    'cclghous': 'college_house_2',\n",
    "    'cstate': 'cstate',\n",
    "    'landline': 'landline',\n",
    "    'hhadult': 'household_adults',\n",
    "    'genhlth': 'general_health',\n",
    "    'physhlth': 'physical_health_days',\n",
    "    'menthlth': 'mental_health_days',\n",
    "    'poorhlth': 'poor_health_days',\n",
    "    'hlthpln1': 'has_health_plan',\n",
    "    'persdoc2': 'has_personal_doctor',\n",
    "    'medcost': 'medical_cost_issue',\n",
    "    'checkup1': 'last_checkup',\n",
    "    'bphigh4': 'high_blood_pressure',\n",
    "    'bpmeds': 'bp_meds',\n",
    "    'bloodcho': 'high_cholesterol',\n",
    "    'cholchk': 'cholesterol_check',\n",
    "    'toldhi2': 'diagnosed_diabetes',\n",
    "    'cvdinfr4': 'had_heart_attack',\n",
    "    'cvdcrhd4': 'had_coronary_heart_disease',\n",
    "    'cvdstrk3': 'had_stroke',\n",
    "    'asthma3': 'has_asthma',\n",
    "    'asthnow': 'asthma_now',\n",
    "    'chcscncr': 'had_skin_cancer',\n",
    "    'chcocncr': 'had_other_cancer',\n",
    "    'chccopd1': 'has_copd',\n",
    "    'havarth3': 'has_arthritis',\n",
    "    'addepev2': 'has_depression',\n",
    "    'chckidny': 'had_kidney_disease'\n",
    "}\n",
    "\n",
    "# Target table dtypes\n",
    "dtype_map = {\n",
    "    'state': Integer(),\n",
    "    'survey_month': Integer(),\n",
    "    'survey_date': Date(),\n",
    "    'month': Integer(),\n",
    "    'day': Integer(),\n",
    "    'year': Integer(),\n",
    "    'disposition_code': Integer(),\n",
    "    'sequence_number': BigInteger(),\n",
    "    'primary_sampling_unit': BigInteger(),\n",
    "    'telephone_number': Integer(),   \n",
    "    'private_residence': Integer(),\n",
    "    'college_house': Integer(),\n",
    "    'state_residence': Integer(),\n",
    "    'cell_phone': Integer(),\n",
    "    'num_living_adults': Integer(),\n",
    "    'num_of_adults': Integer(),\n",
    "    'num_of_men': Integer(),\n",
    "    'num_of_women': Integer(),\n",
    "    'ctelnum1': Integer(),          \n",
    "    'cellfon2': Integer(),\n",
    "    'cadult': Integer(),\n",
    "    'private_residence_2': Integer(),\n",
    "    'college_house_2': Integer(),\n",
    "    'cstate': Integer(),\n",
    "    'landline': Integer(),\n",
    "    'household_adults': Integer(),\n",
    "    'general_health': Integer(),\n",
    "    'physical_health_days': Integer(),\n",
    "    'mental_health_days': Integer(),\n",
    "    'poor_health_days': Integer(),\n",
    "    'has_health_plan': Integer(),\n",
    "    'has_personal_doctor': Integer(),\n",
    "    'medical_cost_issue': Integer(),\n",
    "    'last_checkup': Integer(),\n",
    "    'high_blood_pressure': Integer(),\n",
    "    'bp_meds': Integer(),\n",
    "    'high_cholesterol': Integer(),\n",
    "    'cholesterol_check': Integer(),\n",
    "    'diagnosed_diabetes': Integer(),\n",
    "    'had_heart_attack': Integer(),\n",
    "    'had_coronary_heart_disease': Integer(),\n",
    "    'had_stroke': Integer(),\n",
    "    'has_asthma': Integer(),\n",
    "    'asthma_now': Integer(),\n",
    "    'had_skin_cancer': Integer(),\n",
    "    'had_other_cancer': Integer(),\n",
    "    'has_copd': Integer(),\n",
    "    'has_arthritis': Integer(),\n",
    "    'has_depression': Integer(),\n",
    "    'had_kidney_disease': Integer(),\n",
    "}\n",
    "target_int_cols = [c for c, t in dtype_map.items() if isinstance(t, (Integer, BigInteger))]\n",
    "\n",
    "# --- Global controls ---\n",
    "THRESHOLD = 50.0\n",
    "PROTECT_COLS = {'month', 'day', 'year', 'survey_date'}\n",
    "GLOBAL_DROP_COLS = None   # computed once from first cleaned batch\n",
    "TABLE_INITIALIZED = False # we will REPLACE the table on the first write with kept columns\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (robust parsing & casting)\n",
    "# ----------------------------\n",
    "BYTES_RE = re.compile(r\"^\\s*b\\s*'\\\"['\\\"]\\s*$\")\n",
    "\n",
    "def decode_bytes_literal(x):\n",
    "    if isinstance(x, bytes):\n",
    "        try:\n",
    "            return x.decode('utf-8', errors='ignore')\n",
    "        except Exception:\n",
    "            return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        m = BYTES_RE.match(s)\n",
    "        if m:\n",
    "            return m.group('val')\n",
    "        return s\n",
    "    return x\n",
    "\n",
    "def extract_digits(series: pd.Series, max_len: int) -> pd.Series:\n",
    "    s = series.astype(str).str.strip().map(decode_bytes_literal)\n",
    "    s = s.str.extract(r'(?P<d>\\d{1,' + str(max_len) + r'})', expand=True)['d']\n",
    "    return s.fillna('')\n",
    "\n",
    "def to_int_nullable(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors='coerce').replace([np.inf, -np.inf], np.nan)\n",
    "    integral = s.notna() & np.isfinite(s) & np.isclose(s, np.round(s))\n",
    "    s = s.where(integral, np.nan)\n",
    "    values = [int(v) if (v == v) else pd.NA for v in s]  # v==v is False for NaN\n",
    "    return pd.Series(pd.array(values, dtype='Int64'))\n",
    "\n",
    "def parse_mmddyyyy_to_date(series: pd.Series) -> pd.Series:\n",
    "    s = series.map(decode_bytes_literal).astype(str).str.strip()\n",
    "    s = s.str.extract(r'(?P<digits>\\d{6,8})', expand=True)['digits'].fillna('')\n",
    "    s = s.where(s.eq(''), other=s.str.zfill(8))\n",
    "    dt_vals = pd.to_datetime(s, format='%m%d%Y', errors='coerce')\n",
    "    return dt_vals.dt.date\n",
    "\n",
    "def clean_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.dropna(how='all')\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    for c in ['idate', 'imonth', 'iday', 'iyear']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].map(decode_bytes_literal)\n",
    "\n",
    "    if 'imonth' in df.columns:\n",
    "        df['imonth'] = extract_digits(df['imonth'], 2)\n",
    "        df['imonth'] = to_int_nullable(df['imonth'])\n",
    "    if 'iday' in df.columns:\n",
    "        df['iday'] = extract_digits(df['iday'], 2)\n",
    "        df['iday'] = to_int_nullable(df['iday'])\n",
    "    if 'iyear' in df.columns:\n",
    "        df['iyear'] = extract_digits(df['iyear'], 4)\n",
    "        df['iyear'] = to_int_nullable(df['iyear'])\n",
    "\n",
    "    if 'idate' in df.columns:\n",
    "        df['idate'] = parse_mmddyyyy_to_date(df['idate'])\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    for col in ['telephone_number', 'ctelnum1']:\n",
    "        if col in df.columns:\n",
    "            df[col] = to_int_nullable(df[col])\n",
    "    for col in ['sequence_number', 'primary_sampling_unit']:\n",
    "        if col in df.columns:\n",
    "            df[col] = to_int_nullable(df[col])\n",
    "\n",
    "    for col in target_int_cols:\n",
    "        if col in df.columns and col not in ['sequence_number', 'primary_sampling_unit', 'telephone_number', 'ctelnum1']:\n",
    "            df[col] = to_int_nullable(df[col])\n",
    "\n",
    "    if 'survey_date' in df.columns:\n",
    "        df['survey_date'] = pd.to_datetime(df['survey_date'], errors='coerce').dt.date\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Global drop & schema utilities\n",
    "# ----------------------------\n",
    "def compute_global_drop_cols(df: pd.DataFrame, threshold_pct: float = THRESHOLD) -> list[str]:\n",
    "    null_pct = df.isnull().mean() * 100\n",
    "    to_drop = [c for c in null_pct.index if (null_pct[c] > threshold_pct and c not in PROTECT_COLS)]\n",
    "    print(f\"[GLOBAL] Will drop {len(to_drop)} columns with >{threshold_pct}% nulls:\", to_drop)\n",
    "    return to_drop\n",
    "\n",
    "def fill_month_day_year_from_survey_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'survey_date' in df.columns:\n",
    "        sd = pd.to_datetime(df['survey_date'], errors='coerce')\n",
    "        if 'month' not in df.columns:\n",
    "            df['month'] = sd.dt.month.astype('Int64')\n",
    "        else:\n",
    "            df.loc[df['month'].isna(), 'month'] = sd.dt.month.astype('Int64')\n",
    "        if 'day' not in df.columns:\n",
    "            df['day'] = sd.dt.day.astype('Int64')\n",
    "        else:\n",
    "            df.loc[df['day'].isna(), 'day'] = sd.dt.day.astype('Int64')\n",
    "        if 'year' not in df.columns:\n",
    "            df['year'] = sd.dt.year.astype('Int64')\n",
    "        else:\n",
    "            df.loc[df['year'].isna(), 'year'] = sd.dt.year.astype('Int64')\n",
    "    return df\n",
    "\n",
    "def ensure_target_schema(df: pd.DataFrame, kept_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure df has exactly kept_cols; create missing with appropriate nulls.\n",
    "    \"\"\"\n",
    "    for col in kept_cols:\n",
    "        if col not in df.columns:\n",
    "            # Create appropriate nulls by simple heuristic\n",
    "            if col == 'survey_date':\n",
    "                df[col] = [None] * len(df)\n",
    "            elif col in {'month', 'day', 'year'}:\n",
    "                df[col] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "            elif col in ['sequence_number', 'primary_sampling_unit']:\n",
    "                df[col] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "            elif col in dtype_map and isinstance(dtype_map[col], (Integer, BigInteger)):\n",
    "                df[col] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "            else:\n",
    "                df[col] = pd.Series([pd.NA] * len(df))\n",
    "    # Subset and order\n",
    "    return df[kept_cols]\n",
    "\n",
    "def fill_nulls_dtype_aware(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if num_cols:\n",
    "        df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "    if bool_cols:\n",
    "        df[bool_cols] = df[bool_cols].fillna(False)\n",
    "\n",
    "    dt64_cols = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "    if dt64_cols:\n",
    "        df[dt64_cols] = df[dt64_cols].fillna(pd.Timestamp('1970-01-01'))\n",
    "\n",
    "    if 'survey_date' in df.columns:\n",
    "        mask = df['survey_date'].isna()\n",
    "        if mask.any():\n",
    "            df.loc[mask, 'survey_date'] = dt.date(1970, 1, 1)\n",
    "\n",
    "    obj_cols = [c for c in df.select_dtypes(include=['object']).columns if c != 'survey_date']\n",
    "    if obj_cols:\n",
    "        df[obj_cols] = df[obj_cols].fillna(\"0\")\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Engine should be defined earlier in your script, e.g.:\n",
    "# engine = sa.create_engine(\"postgresql+psycopg2://user:pass@host:port/dbname\")\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Target DB:\", str(getattr(engine, 'url', 'engine (url not available)')))\n",
    "print(f\"Writing to {target_schema}.{target_table}\")\n",
    "\n",
    "rows_total_written = 0\n",
    "\n",
    "# dtype map for kept columns (set after first batch)\n",
    "dtype_map_kept = None\n",
    "kept_cols = None\n",
    "\n",
    "while offset < total_rows:\n",
    "    query = f\"\"\"\n",
    "        SELECT {cols_str}\n",
    "        FROM {source_schema}.{source_table}\n",
    "        ORDER BY SEQNO\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    batch = pd.read_sql_query(query, engine)\n",
    "\n",
    "    if batch.empty:\n",
    "        print(f\"No rows returned at offset {offset}. Stopping.\")\n",
    "        break\n",
    "\n",
    "    before = len(batch)\n",
    "    batch = clean_batch(batch)\n",
    "    after_clean = len(batch)\n",
    "\n",
    "    # Ensure month/day/year populated\n",
    "    batch = fill_month_day_year_from_survey_date(batch)\n",
    "\n",
    "    # Decide global drop on the FIRST cleaned batch only\n",
    "    if GLOBAL_DROP_COLS is None:\n",
    "        GLOBAL_DROP_COLS = compute_global_drop_cols(batch, threshold_pct=THRESHOLD)\n",
    "\n",
    "        kept_cols = [c for c in dtype_map.keys() if c not in GLOBAL_DROP_COLS]\n",
    "        # Protect core columns even if they were mistakenly in drop list\n",
    "        for p in PROTECT_COLS:\n",
    "            if p not in kept_cols and p in dtype_map:\n",
    "                kept_cols.append(p)\n",
    "\n",
    "        # Build kept dtype map\n",
    "        dtype_map_kept = {c: dtype_map[c] for c in kept_cols if c in dtype_map}\n",
    "        print(\"[GLOBAL] Kept columns:\", kept_cols)\n",
    "\n",
    "    # Apply global drop to this batch\n",
    "    if GLOBAL_DROP_COLS:\n",
    "        cols_to_drop_now = [c for c in GLOBAL_DROP_COLS if c in batch.columns]\n",
    "        if cols_to_drop_now:\n",
    "            batch = batch.drop(columns=cols_to_drop_now)\n",
    "\n",
    "    # Enforce stable schema with kept columns only\n",
    "    batch = ensure_target_schema(batch, kept_cols=kept_cols)\n",
    "\n",
    "    # Fill remaining nulls with 0 (dtype-aware)\n",
    "    batch = fill_nulls_dtype_aware(batch)\n",
    "\n",
    "    # Write: on first batch, REPLACE table to physically remove dropped columns in SQL\n",
    "    if not TABLE_INITIALIZED:\n",
    "        write_mode = 'replace'   # re-create table schema with kept columns only\n",
    "        TABLE_INITIALIZED = True\n",
    "        print(f\"[INIT] Creating/replacing {target_schema}.{target_table} with kept columns only...\")\n",
    "    else:\n",
    "        write_mode = 'append'\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        batch.to_sql(\n",
    "            name=target_table,\n",
    "            con=conn,\n",
    "            schema=target_schema,\n",
    "            if_exists=write_mode,\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=100000,\n",
    "            dtype=dtype_map_kept\n",
    "        )\n",
    "\n",
    "    rows_total_written += len(batch)\n",
    "    print(f\"Offset {offset}: fetched {before} -> cleaned {after_clean} -> written {len(batch)}. Total written: {rows_total_written:,}.\")\n",
    "    offset += batch_size\n",
    "\n",
    "print(\"✅ Silver layer processing completed!\")\n",
    "print(f\"Total rows written: {rows_total_written:,}\")\n",
    "\n",
    "# Inspect target schema/tables\n",
    "insp = inspect(engine)\n",
    "schemas = insp.get_schema_names()\n",
    "print(\"Available schemas:\", schemas)\n",
    "if target_schema in schemas:\n",
    "    print(f\"Tables in schema '{target_schema}':\", insp.get_table_names(schema=target_schema))\n",
    "else:\n",
    "    print('Target schema not available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca4fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
